{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/billhensen/uas-nlp-qna-gru?scriptVersionId=238662275\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# QnA GRU","metadata":{}},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"!pip install -q evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T01:55:38.098692Z","iopub.execute_input":"2025-05-09T01:55:38.098974Z","iopub.status.idle":"2025-05-09T01:55:41.447487Z","shell.execute_reply.started":"2025-05-09T01:55:38.098953Z","shell.execute_reply":"2025-05-09T01:55:41.446721Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nimport os\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer\nimport evaluate\nfrom tqdm.auto import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T01:55:41.449049Z","iopub.execute_input":"2025-05-09T01:55:41.44935Z","iopub.status.idle":"2025-05-09T01:55:41.454441Z","shell.execute_reply.started":"2025-05-09T01:55:41.449328Z","shell.execute_reply":"2025-05-09T01:55:41.453642Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_data(\n    dataset_name: str = \"lib3m/lib3m_qa_dataset_v1\",\n    split: str = \"train\",\n    lang: str = \"en\"\n) -> pd.DataFrame:\n    ds = load_dataset(dataset_name, split=split)\n    df = ds.to_pandas()\n    df = df[df.language == lang].reset_index(drop=True)\n    return df\n\n\ndef split_dataframe(\n    df,\n    test_size: float = 0.2,\n    random_state: int = 42\n) -> tuple:\n\n    train_df, val_df = train_test_split(\n        df,\n        test_size=test_size,\n        random_state=random_state,\n        shuffle=True\n    )\n    \n    return train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n\nclass QADataset(Dataset):\n    def __init__(\n        self,\n        dataframe,\n        tokenizer: AutoTokenizer,\n        max_length: int = 512,\n        mode: str = 'generative'\n    ):\n        self.df = dataframe\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.mode = mode\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        question = row['question']\n        content = row['content']\n        answer = row['answer']\n\n        if self.mode == 'extractive':\n            # Cari posisi jawaban di konten\n            start_char = content.find(answer)\n            if start_char == -1:\n                # Jika tidak ditemukan, default ke awal\n                start_char, end_char = 0, 0\n            else:\n                end_char = start_char + len(answer)\n\n            inputs = self.tokenizer(\n                question,\n                content,\n                truncation='only_second',\n                max_length=self.max_length,\n                return_offsets_mapping=True,\n                return_tensors='pt'\n            )\n            offsets = inputs.pop('offset_mapping').squeeze(0)\n\n            # Temukan token start dan end\n            token_start_index = 0\n            token_end_index = 0\n            for i, (s, e) in enumerate(offsets.tolist()):\n                if s <= start_char < e:\n                    token_start_index = i\n                if s < end_char <= e:\n                    token_end_index = i\n                    break\n\n            return {\n                'input_ids': inputs['input_ids'].squeeze(),\n                'attention_mask': inputs['attention_mask'].squeeze(),\n                'start_positions': torch.tensor(token_start_index, dtype=torch.long),\n                'end_positions': torch.tensor(token_end_index, dtype=torch.long)\n            }\n\n        # Generative QA\n        text = f\"<question> {question} <context> {content} <answer>\"\n        tokenized = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        labels = self.tokenizer(\n            answer,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        ).input_ids\n        labels[labels == self.tokenizer.pad_token_id] = -100\n        return {\n            'input_ids': tokenized.input_ids.squeeze(),\n            'attention_mask': tokenized.attention_mask.squeeze(),\n            'labels': labels.squeeze()\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T01:55:43.393181Z","iopub.execute_input":"2025-05-09T01:55:43.393899Z","iopub.status.idle":"2025-05-09T01:55:43.405928Z","shell.execute_reply.started":"2025-05-09T01:55:43.393872Z","shell.execute_reply":"2025-05-09T01:55:43.405236Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"class GRUGenerator(nn.Module):\n    def __init__(self, vocab_size, embed_dim=768, hidden_dim=768, num_layers=2):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, input_ids, attention_mask=None):\n        x = self.embedding(input_ids)\n        outputs, _ = self.gru(x)\n        logits = self.fc(outputs)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T01:55:44.383287Z","iopub.execute_input":"2025-05-09T01:55:44.383575Z","iopub.status.idle":"2025-05-09T01:55:44.388891Z","shell.execute_reply.started":"2025-05-09T01:55:44.383555Z","shell.execute_reply":"2025-05-09T01:55:44.387974Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configurations","metadata":{}},{"cell_type":"code","source":"MODEL_DIR = '/kaggle/working/gru_model'\nBATCH_SIZE = 4\nEPOCHS = 1\nLR = 1e-3\nMAX_LEN = 512\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Use both GPUs\nif torch.cuda.device_count() > 1:\n    MULTI_GPU = True\nelse:\n    MULTI_GPU = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T01:55:48.147022Z","iopub.execute_input":"2025-05-09T01:55:48.147593Z","iopub.status.idle":"2025-05-09T01:55:48.151877Z","shell.execute_reply.started":"2025-05-09T01:55:48.147569Z","shell.execute_reply":"2025-05-09T01:55:48.151256Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"df = load_data()\ntrain_df, val_df = split_dataframe(df)\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\n\ntrain_ds = QADataset(train_df, tokenizer, max_length=MAX_LEN)\nval_ds = QADataset(val_df, tokenizer, max_length=MAX_LEN)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T01:55:50.009057Z","iopub.execute_input":"2025-05-09T01:55:50.009693Z","iopub.status.idle":"2025-05-09T01:55:53.999749Z","shell.execute_reply.started":"2025-05-09T01:55:50.009669Z","shell.execute_reply":"2025-05-09T01:55:53.999117Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"model = GRUGenerator(len(tokenizer), 256, 512).to(DEVICE)\nif MULTI_GPU:\n    model = nn.DataParallel(model)\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\ncriterion = nn.CrossEntropyLoss(ignore_index=-100)\n\n\nos.makedirs(MODEL_DIR, exist_ok=True)\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss = 0\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n    for batch in loop:\n        input_ids = batch['input_ids'].to(DEVICE)\n        labels = batch['labels'].to(DEVICE)\n        optimizer.zero_grad()\n        logits = model(input_ids)\n        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}\")\n    torch.save(model.module.state_dict() if MULTI_GPU else model.state_dict(),\n               f\"{MODEL_DIR}/checkpoint_epoch{epoch+1}.pt\")\n# Save final\ntorch.save(model.module.state_dict() if MULTI_GPU else model.state_dict(), f\"{MODEL_DIR}/final.pt\")\ntokenizer.save_pretrained(MODEL_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T01:55:54.002638Z","iopub.execute_input":"2025-05-09T01:55:54.002863Z","execution_failed":"2025-05-09T01:56:21.625Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"metric_rouge = evaluate.load('rouge')\nmodel.eval()\npreds, refs = [], []\nloop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\nfor batch in loop:\n    input_ids = batch['input_ids'].to(DEVICE)\n    with torch.no_grad():\n        logits = model(input_ids)\n    generated = torch.argmax(logits, dim=-1)\n    for gen_ids, label_ids in zip(generated, batch['labels']):\n        pred = tokenizer.decode(gen_ids.cpu(), skip_special_tokens=True)\n        ref = tokenizer.decode(label_ids[label_ids!=-100].cpu(), skip_special_tokens=True)\n        preds.append(pred)\n        refs.append(ref)\nresults = metric_rouge.compute(predictions=preds, references=refs)\nprint(\"Evaluation (ROUGE):\", results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T01:34:05.468903Z","iopub.execute_input":"2025-05-09T01:34:05.469458Z","iopub.status.idle":"2025-05-09T01:34:06.894033Z","shell.execute_reply.started":"2025-05-09T01:34:05.469441Z","shell.execute_reply":"2025-05-09T01:34:06.893055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}